+++ 
draft = false
date = 2023-02-27T14:16:16+01:00
title = "Normie response to Normie AI Safety Skepticism"
slug = "normie-ai-safety" 
+++

The following post is an adaptation of a
[comment I posted](https://www.reddit.com/r/MachineLearning/comments/11ada91/d_to_the_ml_researchers_and_practitioners_here_do/j9szujb/)
on the [Machine Learning subreddit](https://www.reddit.com/r/MachineLearning/)
in response to a
[post asking whether readers worried about AI Safety/Alignment similarly to Eliezer Yudkowsky](https://www.reddit.com/r/MachineLearning/comments/11ada91/d_to_the_ml_researchers_and_practitioners_here_do/)
(EY). My general impression is that most commenters had not dedicated much
thought to AI Safety (AIS), like most normal people. I personally wouldn't
qualify myself as particularly well-read on the topic either, hence the title.
As I mention towards the end of the article I think there is "alpha" in
discussions happening between us normies of the "out-group" and so I went ahead
and posted that comment and this post for amplification.

It seems like many commenters mostly have issues with the
"[timelines](https://www.lesswrong.com/tag/ai-timelines)", i.e., they do not
think these risks will happen soon enough for us to worry about them now. I
think this is a fair enough stance, there is a lot of uncertainty around
[forecasting](https://www.alignmentforum.org/tag/forecasting-and-prediction) the
future of A(G)I developments.

However I would point out that they should consider that we may all be suffering
from
[exponential growth bias](https://www.bbc.com/future/article/20200812-exponential-growth-bias-the-numerical-error-behind-covid-19).
If indeed AGI is very far away, then great, we indeed have more time to develop
it safely (although I'd be sad to get to live in such a cool future later rather
than sooner). But there is a real chance it isn't, and I do think we need to be
cautious of our confidence in the former case, considering the gravity of the
consequences superintelligence may bring. Basically what I'm saying is "better
safe than sorry", similarly to how we shouldn't dismiss nuclear safety,
epidemiology and shouldn't have dismissed climate alarmists in the 20th century.

The other skepticism I tend to see is that there's no point working on aligning
AI, because even with aligned AI, it will be abused by corporations and
governments, just like our narrower systems are being abused[^1] today. I should
point out that the field of AI Safety
[also considers this problem](https://www.alignmentforum.org/tag/ai-governance)
and posits that solutions to it are also necessary to address safety, similarly
to
[how we coordinate for other security means today](https://en.wikipedia.org/wiki/United_Nations_Security_Council).
See also
[coordination/cooperation](https://www.lesswrong.com/tag/coordination-cooperation).
AI Alignment and AI Governance go hand-in-hand for long-term AI safety and there
are enough people in the world for us to tackle on either without excluding the
other. See
[this image](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/ee952fc01175562d936b528ddf45b2ee7b9f3f8f28214897.png/w_1282)
from
[this post](https://www.lesswrong.com/posts/4basF9w9jaPZpoC8R/intro-to-brain-like-agi-safety-1-what-s-the-problem-and-why).

![AI Alignment and AI Governance go hand-in-hand for long-term
AI-safety.](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/ee952fc01175562d936b528ddf45b2ee7b9f3f8f28214897.png/w_1282)

Finally, I suppose the classic dismissal of AI Safety concerns that I tend to
see is something a long the lines of
"[I don't think AGI will be unsafe](https://www.lesswrong.com/posts/3xACom5ytqBogcuad/chance-that-ai-safety-basically-doesn-t-need-to-be-solved-we)"
or "Existing arguments about AGI being dangerous don't convince me". The first
opinion is basically that we'll get
[Alignment by default](https://www.alignmentforum.org/posts/Nwgdq6kHke5LY692J/alignment-by-default),
without worrying about it now or expending much effort. I think this is
possible, and would love for it to be the case, but tend to go back to my
"better safe than sorry" stance here. As for being unconvinced, I totally
understand, but so far I haven't seen actual arguments or criticisms pointing
out why, and the unconvinced views seem mostly gut-based and/or emotional (which
to be clear, should not be completely dismissed but could benefit from some
argumentation). On the other hand, there are several papers (e.g.
[here](https://arxiv.org/abs/2109.13916) and
[here](https://arxiv.org/abs/2209.00626)), books (e.g.
[here](https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies)
and [here](https://en.wikipedia.org/wiki/Human_Compatible)) and entire websites
(see other links) presenting (mostly) rigorous convincing arguments.

I've been interspersing links throughout this post to highlight that many
people[^2] are actively thinking of the AI Safety issue, and presenting
arguments both for and against worrying about it. Fortunately EY is not the only
person working on it, and in fact many people disagree with him while still
being concerned about AIS. I would actually love to see more _rigorous_
arguments from both sides, but especially from those who decide it is not worth
worrying about, since these are more lacking. I would also love to see more
"outsiders" engaging with the issue (whether they agree with it or not) on
LessWrong, where most discussions on it are happening, since at times it can get
admittedly [in-groupy](https://en.wikipedia.org/wiki/In-group_and_out-group).

[^1]:
    By "abused", I mean used in exploitative ways that overall do not benefit
    (and even harm) local communities or even humanity as a whole. See social
    media.

[^2]: still not enough though imo!
